{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "929c9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('D:\\Repositories\\Thobias\\semantic-segmentation')\n",
    "\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "\n",
    "from constants import *\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d4471b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sem_Dataset(Dataset):\n",
    "    def __init__(self, image_dir, transform):\n",
    "        self.main_dir = image_dir\n",
    "        self.all_img = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        img_loc = os.path.join(self.main_dir, self.all_img[index])\n",
    "        image = Image.open(img_loc)\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_img)\n",
    "\n",
    "\n",
    "def post_process_mask(mask):\n",
    "    \"\"\"\n",
    "    Helper function for automatic mask (produced by the segmentation model) cleaning using heuristics.\n",
    "    \"\"\"\n",
    "\n",
    "    # step1: morphological filtering (helps splitting parts that don't belong to the person blob)\n",
    "    kernel = np.ones((13, 13), np.uint8)  # hardcoded 13 simply gave nice results\n",
    "    opened_mask = cv.morphologyEx(mask, cv.MORPH_OPEN, kernel)\n",
    "\n",
    "    # step2: isolate the person component (biggest component after background)\n",
    "    num_labels, labels, stats, _ = cv.connectedComponentsWithStats(opened_mask)\n",
    "\n",
    "    if num_labels > 1:\n",
    "        # step2.1: find the background component\n",
    "        h, _ = labels.shape  # get mask height\n",
    "        # find the most common index in the upper 10% of the image - I consider that to be the background index (heuristic)\n",
    "        discriminant_subspace = labels[:int(h/10), :]\n",
    "        bkg_index = np.argmax(np.bincount(discriminant_subspace.flatten()))\n",
    "\n",
    "        # step2.2: biggest component after background is person (that's a highly probable hypothesis)\n",
    "        blob_areas = []\n",
    "        for i in range(0, num_labels):\n",
    "            blob_areas.append(stats[i, cv.CC_STAT_AREA])\n",
    "        blob_areas = list(zip(range(len(blob_areas)), blob_areas))\n",
    "        blob_areas.sort(key=lambda tup: tup[1], reverse=True)  # sort from biggest to smallest area components\n",
    "        blob_areas = [a for a in blob_areas if a[0] != bkg_index]  # remove background component\n",
    "        person_index = blob_areas[0][0]  # biggest component that is not background is presumably person\n",
    "        processed_mask = np.uint8((labels == person_index) * 255)\n",
    "\n",
    "        return processed_mask\n",
    "    else:  # only 1 component found (probably background) we don't need further processing\n",
    "        return opened_mask\n",
    "\n",
    "\n",
    "def extract_person_masks_from_frames(processed_video_dir, frames_path, batch_size, segmentation_mask_width, mask_extension):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Currently the best segmentation model in PyTorch (officially implemented)\n",
    "    segmentation_model = models.segmentation.deeplabv3_resnet101(pretrained=True).to(device).eval()\n",
    "    print(f'Number of trainable weights in the segmentation model: {utils.count_parameters(segmentation_model)}')\n",
    "\n",
    "    masks_dump_path = os.path.join(processed_video_dir, 'masks')\n",
    "    processed_masks_dump_path = os.path.join(processed_video_dir, 'processed_masks')\n",
    "    os.makedirs(masks_dump_path, exist_ok=True)\n",
    "    os.makedirs(processed_masks_dump_path, exist_ok=True)\n",
    "\n",
    "    h, w = utils.load_image(os.path.join(frames_path, os.listdir(frames_path)[0])).shape[:2]\n",
    "\n",
    "    if segmentation_mask_width is None:\n",
    "        segmentation_mask_height = h\n",
    "        segmentation_mask_width = w\n",
    "    else:\n",
    "        segmentation_mask_height = int(h * (segmentation_mask_width / w))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((segmentation_mask_height, segmentation_mask_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN_1, std=IMAGENET_STD_1)\n",
    "    ])\n",
    "\n",
    "    dataset = Sem_Dataset('img', transform=transform)\n",
    "    frames_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        processed_imgs_cnt = 0\n",
    "        for batch_id, img_batch in enumerate(frames_loader):\n",
    "            processed_imgs_cnt += len(img_batch)\n",
    "            print(f'Processing batch {batch_id + 1} ({processed_imgs_cnt}/{len(dataset)} processed images).')\n",
    "            img_batch = img_batch.to(device)  # shape: (N, 3, H, W)\n",
    "            result_batch = segmentation_model(img_batch)['out'].to('cpu').numpy()  # shape: (N, 21, H, W) (21 - PASCAL VOC classes)\n",
    "            for j, out_cpu in enumerate(result_batch):\n",
    "                # When for the pixel position (x, y) the biggest (un-normalized) probability\n",
    "                # lies in the channel PERSON_CHANNEL_INDEX we set the mask pixel to True\n",
    "                mask = np.argmax(out_cpu, axis=0) == PERSON_CHANNEL_INDEX\n",
    "                mask = np.uint8(mask * 255)  # convert from bool to [0, 255] black & white image\n",
    "\n",
    "                processed_mask = post_process_mask(mask)  # simple heuristics (connected components, etc.)\n",
    "\n",
    "                filename = str(batch_id*batch_size+j).zfill(FILE_NAME_NUM_DIGITS) + mask_extension\n",
    "                cv.imwrite(os.path.join(masks_dump_path, filename), mask)\n",
    "                cv.imwrite(os.path.join(processed_masks_dump_path, filename), processed_mask)\n",
    "\n",
    "    return {'processed_masks_dump_path': processed_masks_dump_path, 'processed_mask': processed_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad515119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable weights in the segmentation model: 60996202\n",
      "Processing batch 1 (1/1 processed images).\n"
     ]
    }
   ],
   "source": [
    "processed_video_dir = 'proc_vid'\n",
    "frames_path = 'img'\n",
    "segmentation_mask_width = None\n",
    "batch_size = 4\n",
    "mask_extension = '.jpg'\n",
    "\n",
    "processed_mask = extract_person_masks_from_frames(processed_video_dir, frames_path, batch_size, segmentation_mask_width, mask_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0c629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
